# 出会ってから今までに勉強したもの  
## 9月まで（記憶が曖昧）  
* **ROC曲線とPR曲線のAUC** 
  > ROC曲線とPR曲線-分類性能の評価方法を理解する  
  > https://qiita.com/g-k/items/b47b9b0ee2015a3b0b94  
  
  PR曲線はインバランスデータに適用性高いしか覚えてない．．．言うて当時見たサイトってこれじゃないだよな．．．
* **Signateで色々**   
  勉強にはなったよな．．．
## 10月18日まで(LightGBM勉強の流れ)  
  `「Signateの勉強も分類問題と評価指標のところに行ったし，ROC曲線を勉強する際にLightGBMを使ったし，彼女も使ってるし，そうだ！LightGBMを勉強しよう！」という安直な発想から，下記の足搔きが始まってしまった.` 
* **LightGBMとは**  
  > LightGBM 徹底入門 – LightGBMの使い方や仕組み、XGBoostとの違いについて  
  > https://www.codexa.net/lightgbm-beginner/

  上記のサイトを見て，LightGBMの特徴や仕組みを大まかに把握できたけど，「決定木」,「アンサンブル学習」，「勾配ブースティング」を先に勉強しないとアカンやと思い，方向変更．

* **アンサンブル学習と勾配ブースティング** 
  >【機械学習】アンサンブル学習（前編）（後編）
  > https://www.youtube.com/watch?v=0WcrBe017-w  
  > https://www.youtube.com/watch?v=aeb3dgMcF2I
  
  * アンサンブル学習  
    * 複数の学習機を組み合わせ，より良い予測を得ようとするテクニック．  
    * 予測値は「平均を取る」，もしくは「多数決をする」．  
    * 「ブースティング」，「ランダムフォレスト」  

  * バッギング  
    * データセットをn個に分け(ブートストラップ法でデータをサンプリング)，n個の学習器を訓練し，「単純平均」（或いは単純多数決）を取る．

  * スタッキング  
    * モデルごとの重要度を考慮する．  
    * 単純にサンプリングするじゃなくても，特徴量を分けて複数の学習器を訓練するのもあり．
    * 個々の予測値の加重平均を最終的予測値とする．
    
  * ランダムフォレスト（「バギング」+「決定木」）  
    * 訓練データからＮ個のブートストラップデータ集合を取り出す．
    * **これらのデータを使ってＮ個の木T_{n}を生成する． （ここもうちょっと見よう**  
      > この時P個の特徴量からm個だけランダムに選ぶ．
    * 回帰の場合は平均を，分類の場合は多数決を取って最終的な予測値とする．  
    
  * **勾配ブースティング （XGBoostに実装）（ここもうちょっと見よう）**
    * 前の学習器の「残差」に適合する．
    * 詳しい内容は上記のリンクで．    
  
* **決定木とは何ぞや**  
  > [入門]初心者の初心者による初心者のための決定木分析  
  > https://qiita.com/3000manJPY/items/ef7495960f472ec14377  
  >> 決定木とは木構造を用いて分類や回帰を行う機械学習の手法の一つです．  
  >> **分類木と回帰木の総称して決定木といいます．**   
  
  これは勉強になったいいサイト．
  
  今回の予定は二値分類問題のための決定木を勉強することなので，**回帰木を飛ばした．**`（さて次回見返すのがいつになるだろう）`  
  勉強する際に一番疑問を置いた所が「どうやって枝分けポイントを探したの」だった。  
  このさいとでも`この「一番よく分割する素性と閾値の組を選ぶ」というところが肝になってきます．`と書かれている．
  結論かる言うと，  
  > ノード内の不純度を最大限減らす素性と閾値の組を選ぶ．  
  > この組を選び出すのによく使われているのが**エントロピー**と**ジニ不純度**です．  
  
  エントロピー（中文：熵）を用いた4.5アルゴリズムは多分木を構築するため，今回も飛ばした．`(実は当時は勉強したけど，今は記録したくないだけ．跳了但是没有完全跳)`   
  ジニ不純度は計量経済学の分野で社会における所得分配の均衡度を表す指標である．
  決定木を毎回ノードから新たなノードに分けるとき，各新たなノードないのジニ不純度も変化している．  
  二値分類にけるジニ不純度の値は`[0:0.5]`の範囲内にあり，小さいほど不純度が小さくなる（うまく分類出来ている）．  
  そこで，分岐前後の不純度の間の差が利得(gain)と呼ばれ，この指標が枝分けポイントを探すの肝になる．
  簡単に言うと特徴量を一つづつ条件として分類してみて，それぞれもたらした利得（gain）を計算し，一番利得の大きい特徴量を枝分けポイントとする．  
  ジニ不純度の定式化および計算方法は上記のリンクにあるので，忘れたらもう一回計算すればよいでしょ．
  
* **特徴量の重要度**  
  > 決定木アルゴリズムの重要度(importance)を正しく解釈しよう  
  > https://yolo-kiyoshi.com/2019/09/16/post-1226/#outline__1  
  >> **重要度は「ある特徴量で分割することでどれくらいジニ不純度を下げられるのか」を意味しています。**  
  
  上記リンクに書いた通り，LightGBMの重要度計算は主にデフォルトとgainの二種類に分かられている．  
  * デフォルトは単純カウント（この特徴量は条件分けに使った回数）．  
  * gainはジニ不純度における利得の平均で計算されている（決定木の枝分けルールを勉強する際に計算した方法とほぼ一致，サンプル数を割っただけ）．
